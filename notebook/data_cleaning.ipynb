{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dd411dd",
   "metadata": {},
   "source": [
    "## Load needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c818f5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eede912",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "707f8fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv(\"../data/imdb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b75249d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>sentiment</th><th>count</th></tr><tr><td>str</td><td>u32</td></tr></thead><tbody><tr><td>&quot;positive&quot;</td><td>25000</td></tr><tr><td>&quot;negative&quot;</td><td>25000</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 2)\n",
       "┌───────────┬───────┐\n",
       "│ sentiment ┆ count │\n",
       "│ ---       ┆ ---   │\n",
       "│ str       ┆ u32   │\n",
       "╞═══════════╪═══════╡\n",
       "│ positive  ┆ 25000 │\n",
       "│ negative  ┆ 25000 │\n",
       "└───────────┴───────┘"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(pl.col(\"sentiment\").value_counts()).unnest(pl.col(\"sentiment\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "860b2b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace sentiment values with integers\n",
    "# 1 for positive, 0 for negative\n",
    "df = df.with_columns(\n",
    "    pl.col(\"sentiment\").replace(\"positive\", 1).replace(\"negative\", 0).cast(pl.Int8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34469db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>review</th><th>sentiment</th></tr><tr><td>str</td><td>i8</td></tr></thead><tbody><tr><td>&quot;One of the other reviewers has…</td><td>1</td></tr><tr><td>&quot;A wonderful little production.…</td><td>1</td></tr><tr><td>&quot;I thought this was a wonderful…</td><td>1</td></tr><tr><td>&quot;Basically there&#x27;s a family whe…</td><td>0</td></tr><tr><td>&quot;Petter Mattei&#x27;s &quot;Love in the T…</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "┌─────────────────────────────────┬───────────┐\n",
       "│ review                          ┆ sentiment │\n",
       "│ ---                             ┆ ---       │\n",
       "│ str                             ┆ i8        │\n",
       "╞═════════════════════════════════╪═══════════╡\n",
       "│ One of the other reviewers has… ┆ 1         │\n",
       "│ A wonderful little production.… ┆ 1         │\n",
       "│ I thought this was a wonderful… ┆ 1         │\n",
       "│ Basically there's a family whe… ┆ 0         │\n",
       "│ Petter Mattei's \"Love in the T… ┆ 1         │\n",
       "└─────────────────────────────────┴───────────┘"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3823389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "# 80% for training, 20% for testing\n",
    "# Stratified split to maintain the proportion of sentiment classes\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"review\"],\n",
    "    df[\"sentiment\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df[\"sentiment\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1d80c297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>sentiment</th><th>count</th></tr><tr><td>i8</td><td>u32</td></tr></thead><tbody><tr><td>0</td><td>20000</td></tr><tr><td>1</td><td>20000</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 2)\n",
       "┌───────────┬───────┐\n",
       "│ sentiment ┆ count │\n",
       "│ ---       ┆ ---   │\n",
       "│ i8        ┆ u32   │\n",
       "╞═══════════╪═══════╡\n",
       "│ 0         ┆ 20000 │\n",
       "│ 1         ┆ 20000 │\n",
       "└───────────┴───────┘"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f60b02e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_list()\n",
    "X_test = X_test.to_list()\n",
    "y_train = y_train.to_list()\n",
    "y_test = y_test.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c046f66",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c68f8c9",
   "metadata": {},
   "source": [
    "### Text cleaning (remove HTML tags, special characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf10695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove HTML tags from text.\"\"\"\n",
    "    clean = re.compile(\n",
    "        \"<.*?>\"\n",
    "    )  # Regex to match HTML tags, ? indicates non-greedy matching\n",
    "    return re.sub(clean, \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "71ed2b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    \"\"\"Remove special characters from text.\"\"\"\n",
    "    return re.sub(\n",
    "        r\"[^a-zA-Z0-9\\s.,!?\\\"']\", \" \", text\n",
    "    ).lower()  # Keep space and common punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5051d5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing HTML tags and special characters.\"\"\"\n",
    "    text = remove_html_tags(text)\n",
    "    text = remove_special_characters(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7d584b",
   "metadata": {},
   "source": [
    "### Tokenisation\n",
    "\n",
    "The tokenizer from HuggingFace handles vocabulary building, sequence padding, and truncation as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9de9542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained tokenizer. 'bert-base-uncased' is a good general-purpose model.\n",
    "# The 'uncased' means it expects lowercase input, which aligns with our cleaning.\n",
    "# Setting `do_lower_case=False` because we already lowercased the text.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9bde5570",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "UNK_TOKEN_ID = tokenizer.unk_token_id\n",
    "PAD_TOKEN_ID = tokenizer.pad_token_id\n",
    "CLS_TOKEN_ID = tokenizer.cls_token_id  # [CLS] token for classification tasks\n",
    "SEP_TOKEN_ID = tokenizer.sep_token_id  # [SEP] token to separate sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a2cb8de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30522, 100, 0, 101, 102)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE, UNK_TOKEN_ID, PAD_TOKEN_ID, CLS_TOKEN_ID, SEP_TOKEN_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd779944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (670 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "max_l = [len(tokenizer.encode(clean_text(i), add_special_tokens=True)) for i in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a28d2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(561.0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(max_l, 0.90)  # 90th percentile of the sequence lengths\n",
    "# This gives us an idea of the maximum sequence length we might need to handle.\n",
    "# We can use this to set a maximum length for padding/truncation in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a0a02688",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 512  # Set a maximum sequence length for padding/truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26d3418",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for IMDB movie reviews.\n",
    "    Handles text cleaning and tokenization using a Hugging Face tokenizer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Apply cleaning\n",
    "        cleaned_text = clean_text(text)\n",
    "\n",
    "        # Tokenize and encode using Hugging Face tokenizer\n",
    "        # This handles tokenization, numericalization, padding, truncation,\n",
    "        # and attention mask creation.\n",
    "        encoding = self.tokenizer(\n",
    "            cleaned_text,\n",
    "            add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "            max_length=self.max_len,  # Max length for padding/truncation\n",
    "            padding=\"max_length\",  # Pad to max_len\n",
    "            truncation=True,  # Truncate if longer than max_len\n",
    "            return_tensors=\"pt\",  # Return PyTorch tensors\n",
    "        )\n",
    "\n",
    "        # Squeeze to remove the batch dimension added by return_tensors='pt'\n",
    "        # as __getitem__ expects to return single samples.\n",
    "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0f579433",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32  # Set a batch size for DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c092ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset instances\n",
    "train_dataset = IMDBDataset(X_train, y_train, tokenizer, MAX_SEQ_LENGTH)\n",
    "test_dataset = IMDBDataset(X_test, y_test, tokenizer, MAX_SEQ_LENGTH)\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dc33a9",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ba5430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. MultiHeadSelfAttention Module ---\n",
    "# This module implements the self-attention mechanism, allowing the model\n",
    "# to weigh the importance of different words in the input sequence.\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the Multi-Head Self-Attention mechanism.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): The dimension of the input embeddings.\n",
    "        num_heads (int): The number of attention heads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Ensure that d_model is divisible by num_heads\n",
    "        assert d_model % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.embed_dim = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # Linear layers for Query, Key, Value projections\n",
    "        # These project the input into different spaces for each head.\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Output linear layer to combine the outputs of all heads\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for Multi-Head Self-Attention.\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): Input tensor for queries (batch_size, seq_len, d_model).\n",
    "            key (torch.Tensor): Input tensor for keys (batch_size, seq_len, d_model).\n",
    "            value (torch.Tensor): Input tensor for values (batch_size, seq_len, d_model).\n",
    "            mask (torch.Tensor, optional): An optional mask tensor (batch_size, 1, 1, seq_len)\n",
    "                                          to prevent attention to padded tokens.\n",
    "                                          Typically 0 for padded positions, 1 for actual tokens.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after attention (batch_size, seq_len, d_model).\n",
    "            torch.Tensor: Attention weights (batch_size, num_heads, seq_len, seq_len).\n",
    "        \"\"\"\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        # 1. Linear projections for Q, K, V\n",
    "        # Shape after projection: (batch_size, seq_len, d_model)\n",
    "        Q = self.q_proj(query)\n",
    "        K = self.k_proj(key)\n",
    "        V = self.v_proj(value)\n",
    "\n",
    "        # 2. Split into multiple heads and reshape\n",
    "        # Reshape to (batch_size, seq_len, num_heads, head_dim)\n",
    "        # Then permute to (batch_size, num_heads, seq_len, head_dim) for batch matrix multiplication\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # 3. Calculate attention scores (Q @ K_T)\n",
    "        # (batch_size, num_heads, seq_len, head_dim) @ (batch_size, num_heads, head_dim, seq_len)\n",
    "        # -> (batch_size, num_heads, seq_len, seq_len)\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "\n",
    "        # 4. Apply mask (if provided)\n",
    "        # Masking is typically used to ignore padding tokens.\n",
    "        if mask is not None:\n",
    "            # Expand mask to match attention_scores dimensions\n",
    "            # mask shape: (batch_size, 1, 1, seq_len) -> (batch_size, 1, seq_len, seq_len)\n",
    "            # The mask should be broadcastable.\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        # 5. Apply softmax to get attention probabilities\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # 6. Apply dropout to attention weights\n",
    "        attention_weights = self.attn_dropout(attention_weights)\n",
    "\n",
    "        # 7. Multiply attention weights with V\n",
    "        # (batch_size, num_heads, seq_len, seq_len) @ (batch_size, num_heads, seq_len, head_dim)\n",
    "        # -> (batch_size, num_heads, seq_len, head_dim)\n",
    "        context_layer = torch.matmul(attention_weights, V)\n",
    "\n",
    "        # 8. Concatenate heads and reshape back to original embed_dim\n",
    "        # Permute back to (batch_size, seq_len, num_heads, head_dim)\n",
    "        # Then reshape to (batch_size, seq_len, embed_dim)\n",
    "        # The permute() operation can make a tensor non-contiguous.\n",
    "        # Since the subsequent view() operation requires a contiguous tensor to reshape\n",
    "        # .contiguous() is called in between to ensure the memory layout is correct for the view() operation to succeed.\n",
    "        context_layer = (\n",
    "            context_layer.permute(0, 2, 1, 3)\n",
    "            .contiguous()\n",
    "            .view(batch_size, -1, self.embed_dim)\n",
    "        )\n",
    "\n",
    "        # 9. Final linear projection\n",
    "        output = self.out_proj(context_layer)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "# --- 2. PositionalEncoding Module ---\n",
    "# Transformers are permutation-invariant, meaning they don't inherently understand\n",
    "# the order of words. Positional Encoding adds information about the position\n",
    "# of each token in the sequence.\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the Positional Encoding mechanism.\n",
    "    Adds sinusoidal positional encodings to the input embeddings.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): The dimension of the input embeddings.\n",
    "        max_seq_len (int): The maximum sequence length the model is expected to handle.\n",
    "        dropout (float): Dropout rate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, max_seq_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Create a positional encoding matrix\n",
    "        # pe shape: (max_seq_len, d_model)\n",
    "        self.pe = torch.zeros(max_seq_len, d_model)\n",
    "        # position shape: (max_seq_len, 1)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        # div_term shape: (d_moel / 2)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        # Apply sine to even indices in pe, cosine to odd indices\n",
    "        self.pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add an extra dimension for batch (1, max_seq_len, d_model)\n",
    "        # This allows it to be broadcasted to input_embeddings (batch_size, seq_len, d_model)\n",
    "        self.register_buffer(\n",
    "            \"pe\", self.pe.unsqueeze(0)\n",
    "        )  # 'pe' is not a learnable parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for Positional Encoding.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor (batch_size, seq_len, d_model).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor with positional encodings added.\n",
    "        \"\"\"\n",
    "        # Add positional encoding to the input embeddings\n",
    "        # x is (batch_size, seq_len, d_model)\n",
    "        # self.pe is (1, max_seq_len, d_model)\n",
    "        # We slice self.pe to match the current sequence length of x\n",
    "        x = x + self.pe[:, : x.size(1), :]\n",
    "        return self.dropout(x)  # Apply dropout to the output\n",
    "\n",
    "\n",
    "# --- 3. TransformerEncoderLayer Module ---\n",
    "# This is a single layer of the Transformer Encoder, consisting of\n",
    "# Multi-Head Self-Attention, a Feed-Forward Network, and Layer Normalization.\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a single layer of the Transformer Encoder.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): The dimension of the input embeddings.\n",
    "        num_heads (int): The number of attention heads.\n",
    "        ff_dim (int): The dimension of the feed-forward network's hidden layer.\n",
    "        dropout (float): Dropout rate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadSelfAttention(d_model, num_heads, dropout)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, d_model),\n",
    "        )\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for a single Transformer Encoder Layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor (batch_size, seq_len, embed_dim).\n",
    "            mask (torch.Tensor, optional): Attention mask (batch_size, 1, 1, seq_len).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor from this encoder layer.\n",
    "        \"\"\"\n",
    "        attn_output, _ = self.self_attn(\n",
    "            x, x, x, mask\n",
    "        )  # Q, K, V are all from x for self-attention\n",
    "        x = x + self.dropout1(attn_output)  # Residual connection\n",
    "        x = self.norm1(x)  # Layer normalization\n",
    "\n",
    "        ff_output = self.ff(x)  # Feed-forward network\n",
    "        x = x + self.dropout2(ff_output)  # Residual connection\n",
    "        x = self.norm2(x)  # Layer normalization\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# --- 4. TransformerEncoder Module ---\n",
    "# This stacks multiple TransformerEncoderLayer instances to form the full encoder.\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the full Transformer Encoder, stacking multiple Encoder Layers.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Size of the vocabulary.\n",
    "        d_model (int): The dimension of the input embeddings.\n",
    "        num_heads (int): The number of attention heads.\n",
    "        ff_dim (int): The dimension of the feed-forward network's hidden layer.\n",
    "        num_layers (int): The number of TransformerEncoderLayer instances to stack.\n",
    "        max_seq_len (int): The maximum sequence length for positional encoding.\n",
    "        dropout (float): Dropout rate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        d_model,\n",
    "        num_heads,\n",
    "        ff_dim,\n",
    "        num_layers,\n",
    "        max_seq_len,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len, dropout)\n",
    "\n",
    "        # Stack multiple TransformerEncoderLayer instances\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerEncoderLayer(d_model, num_heads, ff_dim, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the Transformer Encoder.\n",
    "\n",
    "        Args:\n",
    "            src (torch.Tensor): Input tensor of token IDs (batch_size, seq_len).\n",
    "            src_mask (torch.Tensor, optional): Source attention mask (batch_size, 1, 1, seq_len).\n",
    "                                              This mask should be 0 for padded tokens.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor from the final encoder layer (batch_size, seq_len, d_model).\n",
    "        \"\"\"\n",
    "        # Convert token IDs to embeddings\n",
    "        x = self.embedding(src)\n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2045cdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transformer-based model for binary sentiment classification.\n",
    "    Consists of a TransformerEncoder followed by a classification head.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        d_model,\n",
    "        num_heads,\n",
    "        ff_dim,\n",
    "        num_layers,\n",
    "        max_seq_len,\n",
    "        dropout=0.1,\n",
    "        num_classes=2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.transformer_encoder = TransformerEncoder(\n",
    "            vocab_size=vocab_size,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            ff_dim=ff_dim,\n",
    "            num_layers=num_layers,\n",
    "            max_seq_len=max_seq_len,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(d_model, num_classes)  # Final classification layer\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the SentimentTransformer.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Tensor of token IDs (batch_size, seq_len).\n",
    "            attention_mask (torch.Tensor): Tensor indicating actual tokens (1) and padding (0)\n",
    "                                          (batch_size, seq_len).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Logits for each class (batch_size, num_classes).\n",
    "        \"\"\"\n",
    "        # Create the mask for the TransformerEncoder.\n",
    "        # The mask needs to be (batch_size, 1, 1, seq_len) for MultiHeadSelfAttention.\n",
    "        # It's typically 0 for padded positions and 1 for actual tokens.\n",
    "        # We convert the attention_mask from (batch_size, seq_len) to (batch_size, 1, 1, seq_len)\n",
    "        # and ensure it's a boolean mask for masked_fill.\n",
    "        src_mask = attention_mask.unsqueeze(1).unsqueeze(2).bool()\n",
    "\n",
    "        # Output shape: (batch_size, seq_len, embed_dim)\n",
    "        encoder_output = self.transformer_encoder(input_ids, src_mask)\n",
    "\n",
    "        # For classification, we typically take the output corresponding to the [CLS] token.\n",
    "        # The [CLS] token is usually the first token in the sequence (index 0).\n",
    "        cls_token_output = encoder_output[:, 0, :]  # Shape: (batch_size, embed_dim)\n",
    "\n",
    "        # Pass the [CLS] token output through the classification head\n",
    "        logits = self.classifier(cls_token_output)  # Shape: (batch_size, num_classes)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50646a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data_loader, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        data_loader (DataLoader): DataLoader for training data.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for updating model parameters.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        device (torch.device): Device to run the model on (CPU or GPU).\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update parameters\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the given data.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # No need to compute gradients during evaluation\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbcbd44",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fda3b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Device configuration (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9955bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 100\n",
      "Test dataset size: 2\n",
      "Number of train batches: 4\n",
      "Number of test batches: 1\n",
      "------------------------------\n",
      "\n",
      "--- Initializing Model, Loss, and Optimizer ---\n",
      "Model initialized with 10183426 trainable parameters.\n",
      "------------------------------\n",
      "\n",
      "--- Starting Training ---\n",
      "Epoch 1/1:\n",
      "  Train Loss: 0.8012, Train Acc: 0.4500\n",
      "  Test Loss: 0.6949, Test Acc: 0.5000\n",
      "------------------------------\n",
      "\n",
      "--- Training Complete! ---\n",
      "Final Test Accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "NUM_EPOCHS = 1\n",
    "LEARNING_RATE = 2e-5  # Common learning rate for Transformers\n",
    "D_MODEL = 256  # Smaller for faster demo, typically 512 or 768\n",
    "NUM_HEADS = 8  # Must divide embed_dim\n",
    "FF_DIM = D_MODEL * 4  # Standard practice\n",
    "NUM_LAYERS = 3  # Number of encoder layers\n",
    "DROPOUT_RATE = 0.1\n",
    "NUM_CLASSES = 2  # Positive/Negative sentiment\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "print(f\"Number of train batches: {len(train_loader)}\")\n",
    "print(f\"Number of test batches: {len(test_loader)}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"\\n--- Initializing Model, Loss, and Optimizer ---\")\n",
    "# Initialize the Sentiment Transformer model\n",
    "model = SentimentTransformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    ff_dim=FF_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    max_seq_len=MAX_SEQ_LENGTH,\n",
    "    dropout=DROPOUT_RATE,\n",
    "    num_classes=NUM_CLASSES,\n",
    ").to(device)  # Move model to the selected device\n",
    "\n",
    "# Define Loss Function (CrossEntropyLoss for classification)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define Optimizer (AdamW is common for Transformers)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\n",
    "    f\"Model initialized with {sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable parameters.\"\n",
    ")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_loss, train_acc = train_model(\n",
    "        model, train_loader, optimizer, criterion, device\n",
    "    )\n",
    "    test_loss, test_acc = evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{NUM_EPOCHS}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "print(\"\\n--- Training Complete! ---\")\n",
    "print(f\"Final Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c3a836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

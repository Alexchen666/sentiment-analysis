{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dd411dd",
   "metadata": {},
   "source": [
    "## Load needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c818f5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eede912",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "707f8fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv(\"../data/imdb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b75249d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>sentiment</th><th>count</th></tr><tr><td>str</td><td>u32</td></tr></thead><tbody><tr><td>&quot;negative&quot;</td><td>25000</td></tr><tr><td>&quot;positive&quot;</td><td>25000</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 2)\n",
       "┌───────────┬───────┐\n",
       "│ sentiment ┆ count │\n",
       "│ ---       ┆ ---   │\n",
       "│ str       ┆ u32   │\n",
       "╞═══════════╪═══════╡\n",
       "│ negative  ┆ 25000 │\n",
       "│ positive  ┆ 25000 │\n",
       "└───────────┴───────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(pl.col(\"sentiment\").value_counts()).unnest(pl.col(\"sentiment\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "860b2b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace sentiment values with integers\n",
    "# 1 for positive, 0 for negative\n",
    "df = df.with_columns(\n",
    "    pl.col(\"sentiment\").replace(\"positive\", 1).replace(\"negative\", 0).cast(pl.Int8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34469db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>review</th><th>sentiment</th></tr><tr><td>str</td><td>i8</td></tr></thead><tbody><tr><td>&quot;One of the other reviewers has…</td><td>1</td></tr><tr><td>&quot;A wonderful little production.…</td><td>1</td></tr><tr><td>&quot;I thought this was a wonderful…</td><td>1</td></tr><tr><td>&quot;Basically there&#x27;s a family whe…</td><td>0</td></tr><tr><td>&quot;Petter Mattei&#x27;s &quot;Love in the T…</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "┌─────────────────────────────────┬───────────┐\n",
       "│ review                          ┆ sentiment │\n",
       "│ ---                             ┆ ---       │\n",
       "│ str                             ┆ i8        │\n",
       "╞═════════════════════════════════╪═══════════╡\n",
       "│ One of the other reviewers has… ┆ 1         │\n",
       "│ A wonderful little production.… ┆ 1         │\n",
       "│ I thought this was a wonderful… ┆ 1         │\n",
       "│ Basically there's a family whe… ┆ 0         │\n",
       "│ Petter Mattei's \"Love in the T… ┆ 1         │\n",
       "└─────────────────────────────────┴───────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3823389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "# 80% for training, 20% for testing\n",
    "# Stratified split to maintain the proportion of sentiment classes\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"review\"],\n",
    "    df[\"sentiment\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df[\"sentiment\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d80c297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>sentiment</th><th>count</th></tr><tr><td>i8</td><td>u32</td></tr></thead><tbody><tr><td>1</td><td>20000</td></tr><tr><td>0</td><td>20000</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 2)\n",
       "┌───────────┬───────┐\n",
       "│ sentiment ┆ count │\n",
       "│ ---       ┆ ---   │\n",
       "│ i8        ┆ u32   │\n",
       "╞═══════════╪═══════╡\n",
       "│ 1         ┆ 20000 │\n",
       "│ 0         ┆ 20000 │\n",
       "└───────────┴───────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f60b02e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_list()\n",
    "X_test = X_test.to_list()\n",
    "y_train = y_train.to_list()\n",
    "y_test = y_test.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c046f66",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c68f8c9",
   "metadata": {},
   "source": [
    "### Text cleaning (remove HTML tags, special characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf10695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove HTML tags from text.\"\"\"\n",
    "    clean = re.compile(\n",
    "        \"<.*?>\"\n",
    "    )  # Regex to match HTML tags, ? indicates non-greedy matching\n",
    "    return re.sub(clean, \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71ed2b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    \"\"\"Remove special characters from text.\"\"\"\n",
    "    return re.sub(\n",
    "        r\"[^a-zA-Z0-9\\s.,!?\\\"']\", \" \", text\n",
    "    ).lower()  # Keep space and common punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5051d5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing HTML tags and special characters.\"\"\"\n",
    "    text = remove_html_tags(text)\n",
    "    text = remove_special_characters(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7d584b",
   "metadata": {},
   "source": [
    "### Tokenisation\n",
    "\n",
    "The tokenizer from HuggingFace handles vocabulary building, sequence padding, and truncation as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9de9542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained tokenizer. 'bert-base-uncased' is a good general-purpose model.\n",
    "# The 'uncased' means it expects lowercase input, which aligns with our cleaning.\n",
    "# Setting `do_lower_case=False` because we already lowercased the text.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bde5570",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "UNK_TOKEN_ID = tokenizer.unk_token_id\n",
    "PAD_TOKEN_ID = tokenizer.pad_token_id\n",
    "CLS_TOKEN_ID = tokenizer.cls_token_id  # [CLS] token for classification tasks\n",
    "SEP_TOKEN_ID = tokenizer.sep_token_id  # [SEP] token to separate sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2cb8de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30522, 100, 0, 101, 102)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE, UNK_TOKEN_ID, PAD_TOKEN_ID, CLS_TOKEN_ID, SEP_TOKEN_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd779944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (670 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "max_l = [len(tokenizer.encode(clean_text(i), add_special_tokens=True)) for i in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a28d2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(561.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(max_l, 0.90)  # 90th percentile of the sequence lengths\n",
    "# This gives us an idea of the maximum sequence length we might need to handle.\n",
    "# We can use this to set a maximum length for padding/truncation in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0a02688",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 512  # Set a maximum sequence length for padding/truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b4312e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline_hf(text, max_len, hf_tokenizer):\n",
    "    \"\"\"\n",
    "    Combines all preprocessing steps for a single text review using Hugging Face tokenizer:\n",
    "    1. Cleans the text.\n",
    "    2. Tokenizes, numericalizes, pads/truncates, and adds special tokens.\n",
    "    3. Returns a dictionary with 'input_ids' and 'attention_mask'.\n",
    "    \"\"\"\n",
    "    cleaned = clean_text(text)\n",
    "    encoding = hf_tokenizer(\n",
    "        cleaned,\n",
    "        add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "        max_length=max_len,  # Max length for padding/truncation\n",
    "        padding=\"max_length\",  # Pad to max_len\n",
    "        truncation=True,  # Truncate if longer than max_len\n",
    "        return_tensors=\"pt\",  # Return PyTorch tensors\n",
    "    )\n",
    "    return encoding  # Returns a BatchEncoding object (like a dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32adc242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all reviews\n",
    "all_input_ids = []\n",
    "all_attention_masks = []\n",
    "for review in X_train:\n",
    "    encoding = text_pipeline_hf(review, MAX_SEQ_LENGTH, tokenizer)\n",
    "    all_input_ids.append(encoding[\"input_ids\"])\n",
    "    all_attention_masks.append(encoding[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a116ee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_ids = []\n",
    "all_test_attention_masks = []\n",
    "for review in X_test:\n",
    "    encoding = text_pipeline_hf(review, MAX_SEQ_LENGTH, tokenizer)\n",
    "    all_test_ids.append(encoding[\"input_ids\"])\n",
    "    all_test_attention_masks.append(encoding[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d276d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all individual tensors into single batch tensors\n",
    "text_input_ids_tensor = torch.cat(all_input_ids, dim=0)\n",
    "text_attention_mask_tensor = torch.cat(all_attention_masks, dim=0)\n",
    "\n",
    "# Convert the test set tensors\n",
    "text_test_input_ids_tensor = torch.cat(all_test_ids, dim=0)\n",
    "text_test_attention_mask_tensor = torch.cat(all_test_attention_masks, dim=0)\n",
    "\n",
    "# Convert labels to a PyTorch tensor\n",
    "labels_tensor = torch.tensor(y_train, dtype=torch.int8)\n",
    "labels_tensor_test = torch.tensor(y_test, dtype=torch.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dc33a9",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ba5430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. MultiHeadSelfAttention Module ---\n",
    "# This module implements the self-attention mechanism, allowing the model\n",
    "# to weigh the importance of different words in the input sequence.\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the Multi-Head Self-Attention mechanism.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): The dimension of the input embeddings.\n",
    "        num_heads (int): The number of attention heads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Ensure that d_model is divisible by num_heads\n",
    "        assert d_model % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.embed_dim = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # Linear layers for Query, Key, Value projections\n",
    "        # These project the input into different spaces for each head.\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Output linear layer to combine the outputs of all heads\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for Multi-Head Self-Attention.\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): Input tensor for queries (batch_size, seq_len, d_model).\n",
    "            key (torch.Tensor): Input tensor for keys (batch_size, seq_len, d_model).\n",
    "            value (torch.Tensor): Input tensor for values (batch_size, seq_len, d_model).\n",
    "            mask (torch.Tensor, optional): An optional mask tensor (batch_size, 1, 1, seq_len)\n",
    "                                          to prevent attention to padded tokens.\n",
    "                                          Typically 0 for padded positions, 1 for actual tokens.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after attention (batch_size, seq_len, d_model).\n",
    "            torch.Tensor: Attention weights (batch_size, num_heads, seq_len, seq_len).\n",
    "        \"\"\"\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        # 1. Linear projections for Q, K, V\n",
    "        # Shape after projection: (batch_size, seq_len, d_model)\n",
    "        Q = self.q_proj(query)\n",
    "        K = self.k_proj(key)\n",
    "        V = self.v_proj(value)\n",
    "\n",
    "        # 2. Split into multiple heads and reshape\n",
    "        # Reshape to (batch_size, seq_len, num_heads, head_dim)\n",
    "        # Then permute to (batch_size, num_heads, seq_len, head_dim) for batch matrix multiplication\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # 3. Calculate attention scores (Q @ K_T)\n",
    "        # (batch_size, num_heads, seq_len, head_dim) @ (batch_size, num_heads, head_dim, seq_len)\n",
    "        # -> (batch_size, num_heads, seq_len, seq_len)\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "\n",
    "        # 4. Apply mask (if provided)\n",
    "        # Masking is typically used to ignore padding tokens.\n",
    "        if mask is not None:\n",
    "            # Expand mask to match attention_scores dimensions\n",
    "            # mask shape: (batch_size, 1, 1, seq_len) -> (batch_size, 1, seq_len, seq_len)\n",
    "            # The mask should be broadcastable.\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        # 5. Apply softmax to get attention probabilities\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # 6. Apply dropout to attention weights\n",
    "        attention_weights = self.attn_dropout(attention_weights)\n",
    "\n",
    "        # 7. Multiply attention weights with V\n",
    "        # (batch_size, num_heads, seq_len, seq_len) @ (batch_size, num_heads, seq_len, head_dim)\n",
    "        # -> (batch_size, num_heads, seq_len, head_dim)\n",
    "        context_layer = torch.matmul(attention_weights, V)\n",
    "\n",
    "        # 8. Concatenate heads and reshape back to original embed_dim\n",
    "        # Permute back to (batch_size, seq_len, num_heads, head_dim)\n",
    "        # Then reshape to (batch_size, seq_len, embed_dim)\n",
    "        # The permute() operation can make a tensor non-contiguous.\n",
    "        # Since the subsequent view() operation requires a contiguous tensor to reshape\n",
    "        # .contiguous() is called in between to ensure the memory layout is correct for the view() operation to succeed.\n",
    "        context_layer = (\n",
    "            context_layer.permute(0, 2, 1, 3)\n",
    "            .contiguous()\n",
    "            .view(batch_size, -1, self.embed_dim)\n",
    "        )\n",
    "\n",
    "        # 9. Final linear projection\n",
    "        output = self.out_proj(context_layer)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "# --- 2. PositionalEncoding Module ---\n",
    "# Transformers are permutation-invariant, meaning they don't inherently understand\n",
    "# the order of words. Positional Encoding adds information about the position\n",
    "# of each token in the sequence.\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the Positional Encoding mechanism.\n",
    "    Adds sinusoidal positional encodings to the input embeddings.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): The dimension of the input embeddings.\n",
    "        max_seq_len (int): The maximum sequence length the model is expected to handle.\n",
    "        dropout (float): Dropout rate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, max_seq_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Create a positional encoding matrix\n",
    "        # pe shape: (max_seq_len, d_model)\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        # position shape: (max_seq_len, 1)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        # div_term shape: (d_moel / 2)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        # Apply sine to even indices in pe, cosine to odd indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add an extra dimension for batch (1, max_seq_len, d_model)\n",
    "        # This allows it to be broadcasted to input_embeddings (batch_size, seq_len, d_model)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))  # 'pe' is not a learnable parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for Positional Encoding.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor (batch_size, seq_len, d_model).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor with positional encodings added.\n",
    "        \"\"\"\n",
    "        # Add positional encoding to the input embeddings\n",
    "        # x is (batch_size, seq_len, d_model)\n",
    "        # self.pe is (1, max_seq_len, d_model)\n",
    "        # We slice self.pe to match the current sequence length of x\n",
    "        x = x + self.pe[:, : x.size(1), :]\n",
    "        return self.dropout(x)  # Apply dropout to the output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
